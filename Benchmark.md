## LLM Models - Comparison Table

**Please note:** This table provides a general overview. Minimum RAM and GPU requirements can vary depending on specific tasks and inference techniques. 

| LLM Model | Number of Parameters (B) | Hugging Face Link | Download Size (GB) | Minimum RAM (GB) | Minimum GPU vRAM (GB) |
|---|---|---|---|---|---|
| Gemma (base) | 7 | [Gemma - Google's new open LLM: Hugging Face](link to Hugging Face) | 1.5 | 8 | 4 |
| microsoft/phi-1_5 | 1.5 | [microsoft/phi-1_5: Hugging Face](link to Hugging Face) | 0.3 | 4 | 2 |
| M4-ai/tau-0.5B | 0.5 | [M4-ai/tau-0.5B: Hugging Face](link to Hugging Face) | 0.1 | 2 | 1 |
| OpenAssistant (Falcon variant) | 40 | OpenAssistant (Falcon variant): Hugging Face |  | Not recommended for local use |  |
| Falcon 40B | 40 | [Falcon: Hugging Face](link to Hugging Face) | Not available for download | Not recommended for local use |  |
| XGen  |  - | [XGen: Hugging Face](link to Hugging Face) | Not available for download | Not recommended for local use |  |
| MPT-30B | 30 | [MPT-30B: Hugging Face](link to Hugging Face) | Not available for download | Not recommended for local use |  |
| Pythia-12B | 12 | [Pythia-12B: Hugging Face](link to Hugging Face) | Not available for download | Not recommended for local use |  |
| RedPajama-INCITE-7B | 7 | [RedPajama-INCITE-7B: Hugging Face](link to Hugging Face) | Not available for download | Not recommended for local use |  |

**Additional Resources:**

* Hugging Face LLM Models: https://huggingface.co/models?other=LLM
* Understanding how big of a model can fit on your machine Hugging Face - Understanding how big of a model can fit on your machine: https://discuss.huggingface.co/t/how-to-get-model-size/11038
* Optimizing your LLM in production Hugging Face - Optimizing your LLM in production: https://discuss.huggingface.co/t/memory-requirements-for-running-llm/57282
